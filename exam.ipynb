{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3MkLaYCJCW6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "data_dir = '/content/drive/MyDrive/seg_pred'  # change to your folder\n",
        "print(os.path.exists(data_dir))\n",
        "print(os.listdir(data_dir)[:30])  # list first 30 entries"
      ],
      "metadata": {
        "id": "TUW43aJPDZTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, os\n",
        "img_paths = glob.glob(os.path.join(data_dir, '**', '*.jpg'), recursive=True)  # jpg example\n",
        "# or include png:\n",
        "# img_paths = glob.glob(os.path.join(data_dir, '**', '*.[jp][pn]g'), recursive=True)\n",
        "len(img_paths)"
      ],
      "metadata": {
        "id": "9GjVCggmDgwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir,            # directory with subfolders per class\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    batch_size=32,\n",
        "    image_size=(224, 224),  # adjust\n",
        "    shuffle=True,\n",
        ")\n",
        "for images, labels in ds.take(1):\n",
        "    print(images.shape, labels.shape)"
      ],
      "metadata": {
        "id": "FwA-KbtXDrFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "SEED = 123"
      ],
      "metadata": {
        "id": "Rzj494IhD31e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.15,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.2  # reserve 20% for validation\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "print(\"Defined train_datagen and val_datagen with validation_split=0.2\")"
      ],
      "metadata": {
        "id": "ZyiuUqDNER_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True,\n",
        "    subset='training',\n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False,\n",
        "    subset='validation',\n",
        "    seed=SEED\n",
        ")"
      ],
      "metadata": {
        "id": "n4MtY7eMEboe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_images, batch_labels = next(train_generator)\n",
        "print(\"Batch images shape:\", batch_images.shape)\n",
        "print(\"Batch labels shape:\", batch_labels.shape)\n",
        "\n",
        "def show_images(images, cols=6):\n",
        "    rows = int(np.ceil(len(images) / cols))\n",
        "    plt.figure(figsize=(cols*2, rows*2))\n",
        "    for i, img in enumerate(images):\n",
        "        plt.subplot(rows, cols, i+1)\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Display first 12 augmented images from this batch\n",
        "show_images(batch_images[:12], cols=6)"
      ],
      "metadata": {
        "id": "YPIYEi4VEefG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why data augmentation matters\n",
        "\n",
        "- Prevents overfitting  \n",
        "  Augmentation makes many slightly different versions of each image. This stops the model from memorizing the exact training images and helps it learn real patterns.\n",
        "\n",
        "- Helps the model work on new images  \n",
        "  Seeing many variations during training teaches the model to handle small changes it will meet in the real world.\n",
        "\n",
        "- Cheap way to get more data  \n",
        "  You donâ€™t need to label more images,augmentation creates extra useful examples from what you already have.\n",
        "\n",
        "Augmentation is an easy, low-cost trick that makes a fine-tuned model more reliable on small datasets."
      ],
      "metadata": {
        "id": "DOkSZUneEvEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: load ResNet50 base (no top) and freeze it\n",
        "input_shape = (224, 224, 3)  # standard for ResNet50\n",
        "\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "base_model.trainable = False  # freeze the whole convolutional base\n",
        "\n",
        "# Optional: show how many layers are trainable (should be zero)\n",
        "trainable_count = sum(1 for layer in base_model.layers if layer.trainable)\n",
        "print(\"ResNet50 base layers:\", len(base_model.layers))\n",
        "print(\"Trainable layers in base:\", trainable_count)"
      ],
      "metadata": {
        "id": "6xwSJ779LDh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: compile the model (ready for training the top-only head)\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss='sparse_categorical_crossentropy',  # or 'categorical_crossentropy' if using one-hot labels\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "print(\"Model compiled. Top layers are trainable; base is frozen.\")"
      ],
      "metadata": {
        "id": "ywFwMxaQH1lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2-YaHwbZLhAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why we freeze early convolutional layers\n",
        "- Early layers learn basic visual patterns like edges, colors and simple textures.  \n",
        "- These basic features are useful across many image tasks, so we keep them fixed to avoid destroying that knowledge.  \n",
        "- Freezing speeds up training and reduces overfitting when we only have a small dataset.  \n",
        "- We train only the top (new) layers so the model quickly learns task-specific patterns for the 6 classes.\n",
        "\n",
        "freeze early layers because they already know useful low-level features and freezing them makes fine-tuning faster and more stable on small datasets."
      ],
      "metadata": {
        "id": "Kbcsg4ojIHN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pathlib\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "DATA_ROOT = \"/content/drive/MyDrive/seg_pred\"  # change as needed\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "SEED = 123\n",
        "TEST_SIZE = 0.1    # fraction for test\n",
        "VAL_SIZE = 0.1     # fraction for validation (from original dataset)\n",
        "\n",
        "data_root = pathlib.Path(DATA_ROOT)\n",
        "\n",
        "# If images are organized as class subfolders:\n",
        "class_dirs = [d for d in data_root.iterdir() if d.is_dir()]\n",
        "class_dirs = sorted(class_dirs)\n",
        "class_names = [d.name for d in class_dirs]\n",
        "print(\"Detected classes:\", class_names)\n",
        "name_to_idx = {name: idx for idx, name in enumerate(class_names)}\n",
        "\n",
        "# Gather all file paths and labels\n",
        "filepaths = []\n",
        "labels = []\n",
        "for class_dir in class_dirs:\n",
        "    for img_path in class_dir.glob('*'):\n",
        "        if img_path.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']:\n",
        "            filepaths.append(str(img_path))\n",
        "            labels.append(name_to_idx[class_dir.name])\n",
        "\n",
        "filepaths = np.array(filepaths)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# First split off test set\n",
        "train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
        "    filepaths, labels, test_size=TEST_SIZE, stratify=labels, random_state=SEED\n",
        ")\n",
        "\n",
        "# Then split train into train and val\n",
        "relative_val_size = VAL_SIZE / (1.0 - TEST_SIZE)  # adjust because test already removed\n",
        "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
        "    train_paths, train_labels, test_size=relative_val_size, stratify=train_labels, random_state=SEED\n",
        ")\n",
        "\n",
        "print(f\"Counts -> train: {len(train_paths)}, val: {len(val_paths)}, test: {len(test_paths)}\")\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_image(path, label):\n",
        "    image = tf.io.read_file(path)\n",
        "    image = tf.image.decode_image(image, channels=3, expand_animations=False)\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)  # scales to [0,1]\n",
        "    image = tf.image.resize(image, IMG_SIZE)\n",
        "    return image, label\n",
        "\n",
        "# Build tf.data datasets\n",
        "def build_dataset(paths, labels, shuffle=True):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=len(paths), seed=SEED)\n",
        "    ds = ds.map(lambda p, l: preprocess_image(p, l), num_parallel_calls=AUTOTUNE)\n",
        "    ds = ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "train_ds = build_dataset(train_paths, train_labels, shuffle=True)\n",
        "val_ds   = build_dataset(val_paths, val_labels, shuffle=False)\n",
        "test_ds  = build_dataset(test_paths, test_labels, shuffle=False)  # keep order stable for predictions\n",
        "\n",
        "# Expose class names\n",
        "print(\"Class names:\", class_names)"
      ],
      "metadata": {
        "id": "Ra4CJxpvLkOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Train only the top head first (assumes `model` built previously and base frozen)\n",
        "# If you haven't created `model` yet, copy the ResNet50 + head creation code from earlier steps.\n",
        "import tensorflow as tf\n",
        "\n",
        "# Confirm which layers are trainable\n",
        "trainable_layers = sum(1 for layer in model.layers if layer.trainable)\n",
        "print(\"Trainable layers before head training:\", trainable_layers)\n",
        "\n",
        "# Compile with a moderate learning rate for the head\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "EPOCHS_HEAD = 2 # change as needed\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
        "]\n",
        "\n",
        "history_head = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS_HEAD,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "veOr4RcsOFAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Unfreeze last N layers of base_model and fine-tune with a low LR\n",
        "# Identify the base model; if you used variable name base_model earlier it exists.\n",
        "N = 30  # number of last layers to unfreeze; tune this\n",
        "# If base_model variable not available, find it inside the model by name or index:\n",
        "try:\n",
        "    base_model\n",
        "except NameError:\n",
        "    # attempt to find first layer that has 'resnet' in its name (common)\n",
        "    for layer in model.layers:\n",
        "        if 'resnet' in layer.name.lower():\n",
        "            base_model = layer\n",
        "            break\n",
        "\n",
        "# More general: if base_model is a Model, set its layers. If base_model is a layer, find model.layers that are in the base.\n",
        "if hasattr(base_model, 'layers'):\n",
        "    for layer in base_model.layers[-N:]:\n",
        "        layer.trainable = True\n",
        "else:\n",
        "    # fallback: unfreeze last N layers of entire model (careful)\n",
        "    for layer in model.layers[-N:]:\n",
        "        layer.trainable = True\n",
        "\n",
        "# Recompile with low learning rate for fine-tuning\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "EPOCHS_FINE = 2  # change as needed\n",
        "history_fine = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS_FINE,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "sylatPeKOHJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Basic evaluation (loss + acc)\n",
        "test_loss, test_acc = model.evaluate(test_ds)\n",
        "print(f\"Test loss: {test_loss:.4f}  Test accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "rRkyWQGMOLQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 12: Generate classification report + confusion matrix together\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Get predictions (probabilities) and true labels\n",
        "y_prob = model.predict(test_ds, verbose=1)\n",
        "y_pred = np.argmax(y_prob, axis=1)\n",
        "\n",
        "# Extract true labels from test_ds (shuffle=False ensured earlier)\n",
        "y_true = np.concatenate([y.numpy() for _, y in test_ds], axis=0)\n",
        "\n",
        "# If dataset yields labels as int tensors, the above works.\n",
        "# Print classification report (text)\n",
        "# Use only the class names that the model was trained on\n",
        "report = classification_report(y_true, y_pred, target_names=[class_names[i] for i in np.unique(y_true)], digits=4)\n",
        "print(\"Classification Report:\\n\")\n",
        "print(report)\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[class_names[i] for i in np.unique(y_true)], yticklabels=[class_names[i] for i in np.unique(y_true)])\n",
        "plt.xlabel(\"Predicted label\")\n",
        "plt.ylabel(\"True label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save figure in case you want a separate image\n",
        "plt.savefig(\"/content/confusion_matrix.png\", dpi=200)\n",
        "print(\"Confusion matrix saved to /content/confusion_matrix.png\")"
      ],
      "metadata": {
        "id": "lqi7Cd4NOMCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: compute predictions (if needed), confusion matrix, and identify top confused pair\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Compute predictions if not already present\n",
        "try:\n",
        "    y_pred  # noqa: F821\n",
        "    print(\"Using existing y_pred\")\n",
        "except NameError:\n",
        "    # predict on test_ds (this may take time)\n",
        "    print(\"Computing predictions on test dataset...\")\n",
        "    y_prob = model.predict(test_ds, verbose=1)\n",
        "    y_pred = np.argmax(y_prob, axis=1)\n",
        "\n",
        "# Get true labels aligned with order of test_paths\n",
        "try:\n",
        "    # Prefer using the array used to build test_ds if available\n",
        "    y_true = np.array(test_labels)  # from the earlier train_test_split\n",
        "    if len(y_true) != len(y_pred):\n",
        "        raise ValueError(\"Length mismatch between test_labels and y_pred\")\n",
        "except Exception:\n",
        "    # Fallback: extract from test_ds (works if shuffle=False)\n",
        "    y_true = np.concatenate([y.numpy() for _, y in test_ds], axis=0)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(\"Confusion matrix shape:\", cm.shape)\n",
        "\n",
        "# Find largest off-diagonal element\n",
        "cm_off = cm.copy().astype(np.int64)\n",
        "np.fill_diagonal(cm_off, -1)  # exclude diagonal\n",
        "max_idx = np.unravel_index(np.argmax(cm_off), cm_off.shape)\n",
        "max_value = cm_off[max_idx]\n",
        "class_a_idx, class_b_idx = max_idx\n",
        "\n",
        "print(f\"Most confused pair: True='{class_names[class_a_idx]}'  Predicted='{class_names[class_b_idx]}'  (count = {max_value})\")\n",
        "# Also print the reverse confusion count for reference\n",
        "reverse_count = cm[class_b_idx, class_a_idx]\n",
        "print(f\"Reverse confusion (True='{class_names[class_b_idx]}' Pred='{class_names[class_a_idx]}') = {reverse_count}\")"
      ],
      "metadata": {
        "id": "OkDH-JBeVZSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: function to show misclassified examples between two classes, then call it for the top confused pair\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "\n",
        "def show_misclassified_examples(paths, y_true, y_pred, class_names,\n",
        "                                class_a_idx, class_b_idx,\n",
        "                                num_examples=6, direction='both', seed=123):\n",
        "    \"\"\"\n",
        "    Display misclassified images between class_a_idx and class_b_idx.\n",
        "\n",
        "    - paths: array-like of file paths (same order as y_true and y_pred)\n",
        "    - direction: 'both' (default) shows A->B and B->A; 'a_to_b' or 'b_to_a' restricts direction.\n",
        "    \"\"\"\n",
        "    paths = np.array(paths)\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    if direction not in ('both', 'a_to_b', 'b_to_a'):\n",
        "        raise ValueError(\"direction must be one of 'both', 'a_to_b', 'b_to_a'\")\n",
        "\n",
        "    if direction == 'both':\n",
        "        mask = ((y_true == class_a_idx) & (y_pred == class_b_idx)) | ((y_true == class_b_idx) & (y_pred == class_a_idx))\n",
        "    elif direction == 'a_to_b':\n",
        "        mask = (y_true == class_a_idx) & (y_pred == class_b_idx)\n",
        "    else:  # 'b_to_a'\n",
        "        mask = (y_true == class_b_idx) & (y_pred == class_a_idx)\n",
        "\n",
        "    indices = np.where(mask)[0]\n",
        "    if len(indices) == 0:\n",
        "        print(\"No misclassified examples found for this pair/direction.\")\n",
        "        return\n",
        "\n",
        "    # sample indices\n",
        "    random.seed(seed)\n",
        "    if len(indices) > num_examples:\n",
        "        indices = random.sample(list(indices), num_examples)\n",
        "    else:\n",
        "        indices = list(indices)\n",
        "\n",
        "    n = len(indices)\n",
        "    ncols = min(3, n)\n",
        "    nrows = math.ceil(n / ncols)\n",
        "    plt.figure(figsize=(4 * ncols, 3.5 * nrows))\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        path = paths[idx]\n",
        "        # Safe image read:\n",
        "        try:\n",
        "            img = plt.imread(path)\n",
        "        except Exception as e:\n",
        "            print(f\"Could not read image at {path}: {e}\")\n",
        "            continue\n",
        "\n",
        "        ax = plt.subplot(nrows, ncols, i + 1)\n",
        "        # If image is float in [0,1] or uint8; imshow handles both.\n",
        "        ax.imshow(img)\n",
        "        ax.axis('off')\n",
        "        true_name = class_names[int(y_true[idx])]\n",
        "        pred_name = class_names[int(y_pred[idx])]\n",
        "        # Show predicted probability if y_prob available\n",
        "        prob_str = \"\"\n",
        "        try:\n",
        "            prob = y_prob[idx]\n",
        "            pred_prob = prob[int(y_pred[idx])]\n",
        "            prob_str = f\"  ({pred_prob:.2f})\"\n",
        "        except Exception:\n",
        "            pass\n",
        "        ax.set_title(f\"True: {true_name}\\nPred: {pred_name}{prob_str}\", fontsize=10)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Call the function for the most confused pair found above\n",
        "# Make sure test_paths exists and aligns with y_true/y_pred\n",
        "try:\n",
        "    test_paths  # noqa: F821\n",
        "except NameError:\n",
        "    raise NameError(\"test_paths not found in the environment. Ensure you have the test_paths array (file paths) available.\")\n",
        "\n",
        "show_misclassified_examples(\n",
        "    paths=test_paths,\n",
        "    y_true=y_true,\n",
        "    y_pred=y_pred,\n",
        "    class_names=class_names,\n",
        "    class_a_idx=class_a_idx,\n",
        "    class_b_idx=class_b_idx,\n",
        "    num_examples=6,\n",
        "    direction='both',\n",
        "    seed=42\n",
        ")"
      ],
      "metadata": {
        "id": "s_hE4BvAVpEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Misclassification Visual Analysis\n",
        "\n",
        "- **True class:** `<CLASS_A>`  \n",
        "- **Predicted as:** `<CLASS_B>`  \n",
        "- **Confusion count:** `<COUNT>`\n",
        "\n",
        "## Observations (visual patterns)\n",
        "- **Backgrounds:** many images share similar backgrounds (e.g., foliage, plain/white, indoor clutter).  \n",
        "- **Color / tone:** dominant similar colors or low contrast across classes (e.g., brown/green, red/orange).  \n",
        "- **Texture / pattern:** large uniform regions or repeating textures that hide fine details.  \n",
        "- **Object scale / crop:** objects are often small, partially visible, or cropped at the image edges.  \n",
        "- **Pose / viewpoint:** similar poses or orientations that remove distinguishing cues.  \n",
        "- **Lighting / exposure:** underexposed or blown highlights reduce visible distinguishing features.  \n",
        "- **Label ambiguity:** some images look ambiguous or contain multiple objects.\n",
        "\n",
        "## Hypothesis\n",
        "The model is confusing these classes because it relies on coarse color/texture and contextual background cues; when discriminative details are missing (small/cropped objects, poor lighting, or ambiguous labels) those low-level cues are shared across both classes causing misclassification.\n",
        "\n",
        "\n",
        "- Inspect and clean ambiguous labels for these two classes.  \n",
        "- Add targeted augmentations (color jitter, random crops, scaling) and more high-res examples.  \n",
        "- Fine-tune higher backbone layers (low LR) so mid-level features can better separate the classes."
      ],
      "metadata": {
        "id": "6uE1I6fxV1F1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save your trained model and class names from Colab\n",
        "# Run this in the same Colab session where `model` and `class_names` exist.\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Path to save model (you can also save to Google Drive: /content/drive/MyDrive/...)\n",
        "SAVE_DIR = \"/content/seg_model_saved\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Save the Keras model\n",
        "# If your model variable is named `model` in Colab, this will save it.\n",
        "print(\"Saving model to:\", SAVE_DIR)\n",
        "model.save(os.path.join(SAVE_DIR, \"my_model.keras\"), include_optimizer=False) # Added .keras extension\n",
        "print(\"Model saved.\")\n",
        "\n",
        "# Save class names to JSON so the Streamlit app can load human-readable labels\n",
        "class_names_path = Path(SAVE_DIR) / \"class_names.json\"\n",
        "with open(class_names_path, \"w\") as f:\n",
        "    json.dump(class_names, f)\n",
        "print(\"Class names saved to:\", class_names_path)\n",
        "\n",
        "# Optional: copy to Google Drive (uncomment and adjust path if you want)\n",
        "# drive_target = \"/content/drive/MyDrive/seg_model_saved\"\n",
        "# !cp -r {SAVE_DIR} {drive_target}\n",
        "# print(\"Copied saved model to Drive:\", drive_target)"
      ],
      "metadata": {
        "id": "Ky0PyXpBXA9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "# Define the path to save the pickle file\n",
        "PICKLE_SAVE_PATH = \"/content/seg_model_saved/my_model.pkl\" # You can change the directory and filename\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(os.path.dirname(PICKLE_SAVE_PATH), exist_ok=True)\n",
        "\n",
        "try:\n",
        "    # Attempt to pickle the model\n",
        "    with open(PICKLE_SAVE_PATH, 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "    print(f\"Model successfully pickled to: {PICKLE_SAVE_PATH}\")\n",
        "    print(\"Note: Pickling Keras models is not the recommended way to save them.\")\n",
        "    print(\"Consider using model.save() in the Keras native format (.keras) or TensorFlow SavedModel format instead.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while pickling the model: {e}\")\n",
        "    print(\"Pickling Keras models can sometimes be problematic.\")\n",
        "    print(\"The recommended way to save your model is using model.save() as demonstrated in the cell above.\")\n",
        "\n",
        "# To load the model later:\n",
        "# try:\n",
        "#     with open(PICKLE_SAVE_PATH, 'rb') as f:\n",
        "#         loaded_model = pickle.load(f)\n",
        "#     print(\"Model successfully loaded from pickle.\")\n",
        "# except Exception as e:\n",
        "#     print(f\"An error occurred while loading the model from pickle: {e}\")"
      ],
      "metadata": {
        "id": "L9Tb1F0zexYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load the model from the correct path and filename\n",
        "model = tf.keras.models.load_model(\"/content/seg_model_saved/my_model.keras\")\n",
        "\n",
        "# Save the model in the SavedModel format (optional, as it's already saved in .keras)\n",
        "# model.save(\"seg_model_saved_tf\")  # Saves as a SavedModel directory\n",
        "\n",
        "print(\"Model loaded successfully.\")"
      ],
      "metadata": {
        "id": "nIVQqffAl-fF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}